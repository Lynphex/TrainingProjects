{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Primero cargo la función para la descarga de imágenes"
      ],
      "metadata": {
        "id": "QywOpX-npq4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dg_SHyxqxO3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from typing import Optional, Union\n",
        "import cv2\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def download_images(paths: list,\n",
        "                    canvas: tuple = (128, 128),\n",
        "                    nb_channels: int = 3,\n",
        "                    max_imgs: Optional[int] = None\n",
        "                    ) -> tuple:\n",
        "    n_images = len(paths) if not max_imgs else max_imgs\n",
        "    images = np.zeros((n_images, canvas[0], canvas[1], nb_channels), dtype=np.uint8)\n",
        "    downloaded_idxs = []\n",
        "\n",
        "    for i_img, url in enumerate(tqdm(paths, total=n_images)):\n",
        "        if i_img >= n_images:\n",
        "            break\n",
        "        try:\n",
        "            img = io.imread(url)\n",
        "            img = cv2.resize(img, (canvas[0], canvas[1]))\n",
        "            downloaded_idxs.append(i_img)\n",
        "            images[i_img] = img\n",
        "        except (IOError, ValueError) as e:  # Unavailable url / conversion error\n",
        "            pass\n",
        "    return images[downloaded_idxs], downloaded_idxs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#descargo el dataset\n",
        "!wget -O \"airbnb-listings.csv\" \"https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\"\n",
        "\n",
        "# Cargo el dataset de airbnb.CSV\n",
        "df = pd.read_csv(\"airbnb-listings.csv\", sep = ';')\n",
        "\n",
        "# Filtro filas sin Thumbnail Url o Precio\n",
        "df = df.dropna(subset=['Thumbnail Url', 'Price'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wxk0tQmxhaU",
        "outputId": "cb550cf4-f8a1-4bd6-8afe-2d8b52fd2911"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-03 22:40:01--  https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\n",
            "Resolving public.opendatasoft.com (public.opendatasoft.com)... 34.248.20.69, 34.249.199.226\n",
            "Connecting to public.opendatasoft.com (public.opendatasoft.com)|34.248.20.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/csv]\n",
            "Saving to: ‘airbnb-listings.csv’\n",
            "\n",
            "airbnb-listings.csv     [             <=>    ]  52.85M  15.3MB/s    in 3.5s    \n",
            "\n",
            "2024-03-03 22:40:05 (15.3 MB/s) - ‘airbnb-listings.csv’ saved [55414009]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargo imágenes\n",
        "\n",
        "downloaded_images, _ = download_images(df['Thumbnail Url'], max_imgs=200)\n",
        "\n",
        "# Elimino columnas innecesarias después de descargar imágenes\n",
        "df = df[['Price', 'Property Type', 'Room Type', 'Cancellation Policy', 'Accommodates',\n",
        "         'Bathrooms', 'Bedrooms', 'Beds', 'Guests Included', 'Extra People',\n",
        "         'Minimum Nights', 'Maximum Nights', 'Number of Reviews', 'Host Total Listings Count']]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENIPwW-7xvGR",
        "outputId": "77bf07d3-3b47-4198-b997-bb2e3a5421b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:08<00:00,  2.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divido en train/val/test\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 60-70% train, resto validation\n",
        "\n",
        "#print(\"El conjunto de entrenamiento tiene dimensiones: \", x_train.shape)\n",
        "#print(\"El conjunto de validación tiene dimensiones: \",x_val.shape)\n",
        "#print(\"El conjunto de test tiene dimensiones: \",x_test.shape)"
      ],
      "metadata": {
        "id": "PeArUhAQxyad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardo\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "val_df.to_csv(\"val_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "# Guardo las imágenes descargadas\n",
        "np.save(\"downloaded_images.npy\", downloaded_images)"
      ],
      "metadata": {
        "id": "Oa4mT0xsx1C-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizo"
      ],
      "metadata": {
        "id": "WVLDXJMIyLtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Selecciono las variables de interés\n",
        "numeric_features = ['Accommodates',\n",
        "         'Bathrooms', 'Bedrooms', 'Beds', 'Guests Included', 'Extra People',\n",
        "         'Minimum Nights', 'Maximum Nights', 'Number of Reviews', 'Host Total Listings Count']\n",
        "\n",
        "categorical_features = ['Property Type', 'Room Type', 'Cancellation Policy']\n",
        "\n",
        "# Elijio el transformador de cada columna\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Añado el preprocesamiento a los conjuntos de datos\n",
        "train_df_numeric_scaled = preprocessor.fit_transform(train_df[numeric_features])\n",
        "val_df_numeric_scaled = preprocessor.transform(val_df[numeric_features])\n",
        "test_df_numeric_scaled = preprocessor.transform(test_df[numeric_features])\n",
        "\n",
        "# Voy a concatenar las características transformadas con el conjunto de datos original\n",
        "train_df = pd.concat([train_df, pd.DataFrame(train_df_numeric_scaled)], axis=1)\n",
        "val_df = pd.concat([val_df, pd.DataFrame(val_df_numeric_scaled)], axis=1)\n",
        "test_df = pd.concat([test_df, pd.DataFrame(test_df_numeric_scaled)], axis=1)\n",
        "\n",
        "# Elimino las columnas originales con variables numéricas que ya no me sirven\n",
        "train_df = train_df.drop(columns=numeric_features)\n",
        "val_df = val_df.drop(columns=numeric_features)\n",
        "test_df = test_df.drop(columns=numeric_features)\n",
        "\n",
        "# Aplico one-hot encoding a las variables categóricas\n",
        "encoder = OneHotEncoder()\n",
        "train_df_cat_encoded = encoder.fit_transform(train_df[categorical_features]).toarray()\n",
        "val_df_cat_encoded = encoder.transform(val_df[categorical_features]).toarray()\n",
        "test_df_cat_encoded = encoder.transform(test_df[categorical_features]).toarray()\n",
        "\n",
        "# las añado transformadas al conjunto de datos original\n",
        "train_df = pd.concat([train_df, pd.DataFrame(train_df_cat_encoded)], axis=1)\n",
        "val_df = pd.concat([val_df, pd.DataFrame(val_df_cat_encoded)], axis=1)\n",
        "test_df = pd.concat([test_df, pd.DataFrame(test_df_cat_encoded)], axis=1)\n",
        "\n",
        "# Elimino las columnas categoricas que ya no me sirven\n",
        "train_df = train_df.drop(columns=categorical_features)\n",
        "val_df = val_df.drop(columns=categorical_features)\n",
        "test_df = test_df.drop(columns=categorical_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "m5qDTuhdyKUt",
        "outputId": "3803934d-958f-4f18-f898-109542d0e473"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A given column is not a column of the dataframe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Property Type'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                 \u001b[0mcol_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Property Type'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-70a44b39d826>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Añado el preprocesamiento a los conjuntos de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_df_numeric_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mval_df_numeric_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_df_numeric_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcolumn_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo las imágenes descargadas\n",
        "downloaded_images = np.load(\"downloaded_images.npy\")\n",
        "\n",
        "# Aquí hago la normalización y redimensionado\n",
        "images = downloaded_images.astype(\"float32\") / 255.\n",
        "image_size = (128, 128)\n",
        "\n",
        "# Guardo imágenes normalizadas\n",
        "np.save(\"normalized_images.npy\", images)\n",
        "\n",
        "\n",
        "# Dividir en train/val/test\n",
        "train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "train_images, val_images = train_test_split(train_images, test_size=0.25, random_state=42)\n"
      ],
      "metadata": {
        "id": "IzW1Ik6zyVeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Guardar los archivos en Google Drive\n",
        "!cp train_data.csv \"/content/drive/My Drive/\"\n",
        "!cp val_data.csv \"/content/drive/My Drive/\"\n",
        "!cp test_data.csv \"/content/drive/My Drive/\"\n",
        "!cp normalized_images.npy \"/content/drive/My Drive/\"\n",
        "!cp thumbnails/train/train_images.npy \"/content/drive/My Drive/\"\n",
        "!cp thumbnails/validation/val_images.npy \"/content/drive/My Drive/\"\n",
        "!cp thumbnails/test/test_images.npy \"/content/drive/My Drive/\"\n"
      ],
      "metadata": {
        "id": "F81q9GLs2wsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah images* filtered* #compruebo que se haya hecho bien"
      ],
      "metadata": {
        "id": "3r6r_L1-LQ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Creo modelo paso a paso\n",
        "tabular_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)  # Una neurona para la regresión\n",
        "])\n",
        "\n",
        "tabular_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "tabular_model.fit(train_df[features], train_df['Price'], epochs=10, batch_size=32, validation_data=(val_df[features], val_df['Price']))"
      ],
      "metadata": {
        "id": "MF67-hhKCYU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# Cargo un modelo base preentrenado\n",
        "base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), include_top=False, weights='imagenet')\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Crear modelo\n",
        "image_model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)  # Una neurona para la regresión\n",
        "])\n",
        "\n",
        "# Compilar modelo\n",
        "image_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Entrenamiento del modelo de imágenes\n",
        "image_model.fit(train_images, epochs=10, validation_data=val_images)\n",
        "\n",
        "#Grafo de la  pérdida\n",
        "plt.plot(np.arange(0, n_epochs), loss_epoch_tr)\n",
        "plt.plot(np.arange(0, n_epochs), loss_epoch_val)\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')"
      ],
      "metadata": {
        "id": "TJdVBJaLCjS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vZtQeovRCoVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nIV_T7eDAaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}